from document import Document

sample_sentence = {
    (0.0, 4.99): "The quick brown",
    (5.0, 9.99): "fox jumps over",
    (10.0, 14.99): "the lazy dog.",
}

sample_sentence2 = [
    {
        "text": "Lorem ipsum",
        "start": 15.0,
        "end": 19.99,
    },
    {
        "text": "dolor sit amet,",
        "start": 20.0,
        "end": 24.99,
    },
    {
        "text": "consectetur adipiscing elit.",
        "start": 25.0,
        "end": 29.99,
    },
]

sample_document = {
    0: sample_sentence,
    1: sample_sentence2,
}

sample_document2 = [{(0.0, 13.6): ' Good morning, everyone.'}, {(13.6, 17.76): ' Welcome to the 13th lecture of 6.006.'}, {(17.76, 22.32): ' Just to recap from last time,', (22.32, 28.56): " we've been talking about single source shortest paths on weighted graphs for", (28.56, 35.0): ' the past two lectures. Previously, we were only talking about unweighted graphs. And', (35.0, 41.78): " so far, up until today, we've talked about three ways to solve single source shortest", (41.78, 48.0): ' paths on weighted graphs. Namely, the first one, use BFS if you', (48.0, 51.36): ' can kind of transform your graph into a linear size', (51.36, 55.2): " graph that's unweighted that corresponds", (55.2, 59.54): ' to your weighted problem, essentially replacing', (59.54, 65.88): ' each weighted edge of weight w with w single edges.'}, {(65.88, 69.38): " Now, that's only good for positive weight things", (69.38, 73.68): ' and if the sum of your weights are small.'}, {(73.68, 75.84): ' But if the sum of your weights is', (75.84, 80.14): ' linear in the combinatorial size of your graph, v plus e,', (80.14, 82.32): ' then we can get a linear time algorithm', (82.32, 86.36): ' to solve weighted shortest paths using breadth-first search.'}, {(88.04, 99.28): ' Then we talked about how we could, if we, the problem with weighted shortest paths is if our weights were negative and there could exist cycles,', (99.58, 106.76): ' then we could have negative weight cycles and that would be more difficult to handle because then you have vertices where you have an unbounded number of edges you might have negative weight cycles. And that would be more difficult to handle, because then you have vertices where', (106.76, 109.0): ' you have an unbounded number of edges', (109.0, 111.84): ' you might have to go through for a shortest path.'}, {(111.84, 115.3): ' There might not be a finite length shortest path.'}, {(115.3, 118.0): " But in the condition where we didn't have cycles", (118.0, 119.66): " in the graph, of course, we couldn't", (119.66, 121.5): ' have negative weight ones.'}, {(121.5, 125.74): ' So we are also able to do that in linear time', (125.74, 129.4): ' by exploiting the fact that our vertices could be ordered', (129.4, 133.44): ' in a topological order, and that we could kind of push', (133.44, 137.94): ' shortest path information from the furthest one back', (137.94, 141.8): ' to the ones forward by relaxing edges forward,', (141.8, 148.5): ' by maintaining this invariant, that we had shortest paths as we were processing these things', (148.5, 151.34): ' in topological order.'}, {(151.34, 155.16): ' Then last time, we were talking about general graphs, graphs', (155.16, 156.26): ' that could contain cycles.'}, {(156.26, 158.82): ' And this is our most general algorithm,', (158.82, 161.5): ' because if there are negative weight cycles,', (161.5, 163.94): ' Bellman-Ford, which we talked about last time,', (163.94, 165.16): ' can detect them.'}, {(165.16, 168.7): ' And in particular, for any vertex', (168.7, 174.22): ' that had a finite weight shortest path,', (174.22, 178.4): ' we could compute a shortest path for it, compute its distance.'}, {(178.4, 181.0): ' And for any one that is reachable from a negative weight', (181.0, 187.62): ' cycle, not only could we mark it as minus infinity distance, but we could', (187.62, 194.94): ' also find a negative weight cycle essentially by duplicating our graph to make it a DAG and being', (194.94, 208.84): " able to follow pointers back in this expanded DAG that had multiple layers. So that's what we've done up until now. We've gotten linear for some types of graphs."}, {(208.84, 216.1): " And we've gotten kind of quadratic, v times e, for general graphs, ones that could contain", (216.1, 217.1): ' negative cycles.'}, {(217.1, 219.94): ' Now, how bad is this?', (219.94, 225.96): ' Well if the graph is sparse, if the number of edges', (225.96, 228.76): ' in our graph is on the order of v,', (228.76, 232.38): ' then this is quadratic time in v, v squared.'}, {(232.38, 240.14): ' But if the graph is dense, where we have the complete graph', (240.14, 242.02): ' where every edge is present, then we', (242.02, 245.36): ' have quadratically many edges in our graph in v.'}, {(245.92, 251.36): ' And so this running time is v cubed. v cubed is not great in terms of its running time. We would', (251.36, 260.64): " like something closer to linear. And so that's what we're going to do today. If we have this", (260.64, 266.94): " restriction where we have non-negative weights, we can't have negative weight cycles."}, {(266.94, 272.0): ' And this is a restriction that comes up a lot for many graphs', (272.0, 274.02): ' you might encounter.'}, {(274.02, 278.14): " A lot of times, you don't have both positive and negative", (278.14, 278.7): ' weights.'}, {(278.7, 283.4): " I don't have a negative distance to my house."}, {(283.4, 287.74): ' In any metric, we have non-negative weights.'}, {(287.74, 290.42): ' So these things come up a lot.'}, {(290.42, 292.76): ' And we can actually do quite a bit better', (292.76, 295.06): ' since there are no negative weight cycles.'}, {(295.06, 298.64): ' We can get almost linear.'}, {(298.64, 302.14): " It's not going to be quite v plus e."}, {(302.14, 307.26): " As you see up here on the slide, we're going to get something very close."}, {(307.26, 310.72): " It's v plus e, but on the v term,", (310.72, 313.92): ' we have this logarithmic factor in v, which remember,', (313.92, 317.7): ' for all intents and purposes, this log of that thing', (317.7, 322.02): ' in real life is not going to be bigger than a factor of 30', (322.02, 325.64): ' or something like that, maybe 60.'}, {(325.64, 327.92): " But it's a small number."}, {(327.92, 330.82): ' And so this is actually pretty good performance.'}, {(330.82, 331.9): " It's almost linear."}, {(331.9, 334.1): " That's what I'm saying, almost linear here."}, {(334.1, 337.36): " And that's what we're going to try to do today."}, {(337.36, 340.48): ' So how do we do this?', (340.48, 350.58): " Well, I'm going to make two observations here. First off, our idea is going to be to generalize the notion of BFS."}, {(350.58, 359.22): ' When we had BFS, we split up our graph to solve weighted shortest paths with BFS.'}, {(359.22, 366.72): ' We could take our positive edge weights, break them up into individual edges.'}, {(366.72, 373.28): " But if the total weight of our edges was large, then we'd have a problem because now we've", (373.28, 375.66): ' expanded the size of our graph.'}, {(375.66, 381.26): " This is the same issue that we had with something like radix sort, where we don't want our algorithm", (381.26, 388.64): ' to run in the size of the numbers in our input. We want our algorithm to run in the number of numbers', (388.64, 389.68): ' in our input.'}, {(389.68, 394.18): ' This is the difference between n and u,', (394.18, 396.3): ' back when we were talking about data structures.'}, {(396.3, 402.98): ' Here, if the size of our weights are large compared to v and e,', (402.98, 406.66): ' then doing this expansion', (406.66, 411.98): ' is going to be difficult. But if we had, say, some graph,', (411.98, 415.98): ' this is my graph g, and we had a source vertex s,', (415.98, 419.86): ' the idea here is going to still be', (419.86, 425.44): ' to try to grow a frontier of increasing distance', (425.44, 432.9): ' from my source and try to maintain all of the things', (432.9, 436.5): ' within a certain distance from my source.'}, {(436.5, 437.28): " So that's the idea."}, {(437.28, 439.26): ' Grow a sphere centered at my source,', (439.26, 443.58): ' repeatedly explore closer vertices', (443.58, 445.1): ' before I get to further ones. But how can I explore closer vertices before I get to further ones,', (445.1, 449.32): ' but how can I explore closer vertices', (449.32, 452.88): " if I don't know the distances beforehand?", (452.88, 456.08): ' This seems like a circular logic.'}, {(456.08, 458.66): " I'm going to use the distance to my things", (458.66, 461.32): ' to compute the distances to my things.'}, {(461.32, 463.78): " That doesn't work so well."}, {(463.78, 465.32): ' So how do we do this?', (465.32, 469.44): ' Well, the idea here is to gradually compute', (469.44, 472.84): ' the distances, compute the distances as we go,', (472.84, 474.52): ' so that we maintain this property.'}, {(474.52, 479.68): " Now, this property, this idea wouldn't work necessarily", (479.68, 483.16): ' in the context of negative edge weights.'}, {(483.16, 487.4): ' Here, we have this growing frontier, this ball,', (487.4, 489.3): ' around my source.'}, {(489.3, 493.26): ' And as I grow my thing, these things', (493.26, 495.3): ' are at further and further distance,', (495.3, 500.2): ' because any edge from something back here,', (500.2, 502.72): " as I'm growing my ball a certain distance,", (502.72, 504.96): ' these things are outside that distance.'}, {(504.96, 508.3): " We're kind of using a key observation here."}, {(508.3, 510.08): " Here's my observation one."}, {(515.12, 530.08): ' If weights greater than or equal to 0, then distances', (530.08, 536.96): ' increase along shortest paths.'}, {(541.16, 544.32): ' Maybe weakly monotonically increase', (544.32, 545.54): ' if there are 0-weight edges.'}, {(545.66, 556.02): ' But in general, if I had a path going from S to some V,', (558.02, 562.96): " and it's going through some vertex U, right?", (562.96, 564.38): ' I have some shortest path.'}, {(564.48, 567.12): ' This is the shortest path from s to v.'}, {(567.12, 571.32): ' And it goes through some point u, some vertex u.'}, {(571.32, 574.4): ' Then this monotonicity, more specifically,', (574.4, 580.38): ' means that the shortest path from s to u', (580.38, 591.0): ' and the shortest path from s to V, which is this whole thing,', (591.0, 594.62): ' how do these relate to each other?', (594.62, 597.68): ' If this is along that path, then this', (597.68, 601.88): ' has to be at least as large as the subpath,', (601.88, 605.04): ' because all of these, the weight of this path', (605.04, 607.0): ' cannot be negative.'}, {(607.0, 610.02): " So that's the thing that Dijkstra is going to exploit."}, {(610.02, 611.64): " It essentially means that when I'm", (611.64, 618.1): ' expanding this frontier of distance away from x,', (618.1, 624.66): " it's possible if I had negative weight that this line,", (624.66, 629.8): ' if I had some very negative weight going from a vertex here to a vertex here,', (631.3, 647.32): ' this vertex could be within this boundary. Maybe if this distance is x, this guy could be within x. The things that are within distance x of s', (647.32, 649.88): ' might not be all contained.'}, {(649.88, 654.4): ' There could be a path from here to this other vertex', (654.4, 657.76): " with distance x that doesn't have this property", (657.76, 660.88): ' because I could decrease in distance along the path.'}, {(660.88, 663.4): " OK, so that's the first observation."}, {(663.4, 668.98): " Second observation, well, let's see", (668.98, 675.6): ' if we can piggyback on DAG relaxation.'}, {(675.6, 693.94): " I claim to you that we can solve single-source shortest paths faster if we're given order of vertices", (693.94, 708.24): ' in increasing distance beforehand, distance from s.'}, {(708.24, 709.58): " So here's the idea."}, {(709.58, 711.36): " I'm not going to give you the distances", (711.36, 714.9): ' to all these vertices.'}, {(714.9, 717.54): " Instead, I'm going to give you the order", (717.54, 722.8): ' of the vertices in some increasing distance from s.'}, {(722.8, 729.92): " So basically, I'm saying if I had some, I don't know, here's a graph."}, {(729.92, 756.24): " Let's see if I can remember. here."}, {(762.96, 764.2): " OK, and I'm going to call these vertices 0, 1, 2, 3, and 4. OK, so here's a graph."}, {(766.78, 769.96): ' Maybe I put some edge weights on here.'}, {(769.96, 777.94): " I'm going to say this one is 3, this one is 2, this one is 3,", (777.94, 785.0): ' this is 1, this is 1, this is 0, and this is 0.'}, {(785.0, 790.56): ' So from vertex 1 to 2, that was a 2', (790.56, 795.3): ' for the labeling of that vertex, that edge is 0 weight.'}, {(795.3, 797.98): " So here's a weighted graph."}, {(797.98, 801.0): " And I don't necessarily know."}, {(801.0, 803.38): ' I could use Bellman-Ford to find shortest paths', (803.38, 809.54): ' from this vertex 0, right?', (809.54, 812.26): " But the idea here is I'm not going to give you shortest", (812.26, 812.8): ' paths.'}, {(812.8, 814.84): " I'm going to try to compute shortest paths."}, {(814.84, 816.88): " But I'm going to give you some additional information."}, {(816.88, 819.78): " I'm going to give you the order of their shortest path", (819.78, 825.62): ' distance from the source.'}, {(825.62, 832.48): " And I'm going to eyeball this and say,", (832.48, 835.46): " I'm going to change this slightly to make it a little bit", (835.46, 836.66): ' more interesting.'}, {(836.66, 840.26): " I'm going to say this is distance 4."}, {(840.26, 842.76): ' OK.'}, {(842.76, 846.42): ' All right, so now what we have is the shortest path distance.'}, {(846.42, 847.74): " I'm just eyeballing this."}, {(847.74, 857.88): ' The shortest path distance to bad example.'}, {(857.88, 858.54): ' All right.'}, {(858.54, 861.88): ' So these are the weights.'}, {(861.88, 865.28): ' Shortest path distance to 3 is going to be 2,', (865.28, 868.64): " I'm going to say, through there."}, {(868.64, 873.2): ' Shortest path distance here is 2 also.'}, {(873.2, 875.54): ' Shortest path distance here is also 2,', (875.54, 877.46): ' because I can go through both of these 0s,', (877.46, 880.12): " and it's not a problem."}, {(880.12, 883.12): ' And then the shortest path distance here is 2 to here', (883.12, 885.88): ' and 1 third to there.'}, {(885.88, 890.92): ' So these are listed in increasing distance', (890.92, 893.12): ' from my source.'}, {(893.12, 898.84): ' I had to compute those deltas to kind of convince you', (898.84, 900.38): ' that this is the right ordering.'}, {(900.38, 903.12): ' But this is a right ordering of these things.'}, {(903.12, 905.84): " Now, it's not the only right ordering. But it is a right ordering of these things. Now, it's not the only right ordering, right?", (906.26, 907.7): ' But it is a right order.'}, {(907.9, 914.46): " OK, so I'm arguing to you that I could solve single source shortest paths in linear time", (914.46, 918.24): ' if I were to give you the vertices in increasing distance.'}, {(918.54, 931.68): ' How could I know that if these are increasing in distance,', (931.68, 938.12): " any edge going backwards with respect to this ordering can't participate in shortest paths", (938.12, 941.06): ' with one exception.'}, {(941.06, 951.16): ' Anyone know what that exception is? No edge can go backwards in this ordering based on this observation, except under what', (951.16, 952.16): ' condition?', (952.16, 953.16): ' Yeah?', (953.16, 954.16): ' If the weight is 0.'}, {(954.16, 955.54): ' If the weight is 0, yeah.'}, {(955.54, 963.78): ' So if the weight is 0, just like this situation here, then I could go backwards in the ordering.'}, {(963.78, 965.26): ' Seems problematic, right?', (965.38, 968.62): " The idea is I'm going to want to construct a DAG", (968.62, 971.0): ' so that I can run DAG relaxation.'}, {(971.88, 976.2): ' Well, if I have a component here', (976.2, 980.54): ' that has zero weights,', (981.8, 986.42): ' I can kind of coalesce this thing down.'}, {(986.42, 991.4): ' I can deal with this component separately.'}, {(991.4, 993.8): " Let's worry about that separately."}, {(993.8, 1000.16): ' If we do, we can collapse this edge down into a single vertex', (1000.16, 1010.76): " and transform this graph so it does respect the ordering. So I'm going to transform this graph into a new graph."}, {(1010.76, 1016.74): ' This is a graph that contains vertex 2 and vertex 0,', (1016.74, 1023.46): ' vertex 1 and 3 here, and vertex 4.'}, {(1023.46, 1026.34): " Now we have, and I'm only going", (1026.34, 1035.36): ' to keep edges going forward in the,', (1035.36, 1041.32): " I'm going to need to collapse this entire section,", (1041.32, 1045.0): ' this entire section down into one vertex.'}, {(1045.0, 1047.0): " This doesn't quite work."}, {(1047.0, 1048.0): ' OK.'}, {(1048.0, 1051.0): " Let's ignore zero-weight edges for now."}, {(1051.0, 1056.0): " Let's assume these are..."}, {(1056.0, 1061.0): ' Hmm, hmm, hmm.'}, {(1061.0, 1063.0): " All right. There's something broken here."}, {(1063.0, 1071.84): " If I have a cycle here, right now I don't have a cycle of 0 weight."}, {(1071.84, 1077.76): ' So what I could do is I could take this vertex and put it after both of these vertices.'}, {(1077.76, 1083.12): " And now I could rearrange the order of these three vertices where there's a path of length", (1083.12, 1087.38): ' 0 and get a new ordering that still satisfies', (1087.38, 1089.84): ' the property.'}, {(1089.84, 1097.64): " And that's always the case because paths", (1097.64, 1099.88): " can't decrease in weight."}, {(1099.88, 1102.36): ' I can rearrange the ordering of these things', (1102.36, 1106.84): ' so that 3 comes first, 1 comes second,', (1106.84, 1109.66): ' and 2 comes third of those three vertices.'}, {(1113.44, 1114.16): ' Yeah.'}, {(1114.16, 1120.52): ' So for every set of 0 edges, I can just', (1120.52, 1126.0): ' flip the relationship if they have the same distance.'}, {(1126.0, 1130.56): " In my input, I'm given vertices that have the same distance", (1130.56, 1131.5): ' from the source.'}, {(1131.5, 1133.9): ' And so if those are the same distance from the source', (1133.9, 1136.06): " and they're connected by a 0-weight edge,", (1136.06, 1137.86): " it doesn't hurt me to flip their ordering."}, {(1137.86, 1139.94): " So I'm going to do that."}, {(1139.94, 1142.56): " So let's convert that into a graph", (1142.56, 1153.06): ' with a different ordering.'}, {(1159.48, 1172.14): ' 0, 3 now, 1, 2. And I have this distance, this edge, this edge, this edge, this edge, this edge.'}, {(1172.14, 1173.14): ' What am I missing?', (1173.14, 1176.92): ' 2 to 3.'}, {(1176.92, 1178.58): ' And here.'}, {(1178.58, 1181.52): ' I think I have all of those edges.'}, {(1181.52, 1183.24): ' Yeah?', (1183.24, 1190.46): ' OK. Now I have the property that every edge that could participate in the shortest path are', (1190.46, 1193.84): ' going forward in the ordering, right?', (1193.84, 1199.64): ' Because all of these are zero weight.'}, {(1199.64, 1209.6): " So we flip those around so they're going correct with respect to the ordering. And any edge going backwards that is positive weight", (1209.6, 1212.38): " certainly can't be used in any shortest path."}, {(1212.38, 1214.46): " So I'm just going to get rid of them."}, {(1217.8, 1218.3): ' Yeah?', (1218.3, 1221.04): " What do I do if there's a zero weight cycle?", (1221.04, 1223.0): " JASON KUOCHIKA- If there's a zero weight cycle,", (1223.0, 1225.88): ' I can just coalesce them all together down to a single', (1225.88, 1229.76): ' vertex because if I reach one of them, I can reach all of them.'}, {(1229.76, 1230.76): ' OK?', (1230.76, 1233.24): " You're computing a topological order."}, {(1233.24, 1234.24): ' Exactly.'}, {(1234.24, 1236.0): " I'm computing a topological."}, {(1236.0, 1239.46): " So the idea here is we're trying to construct a DAG."}, {(1239.46, 1243.22): ' I can construct this DAG in linear time.'}, {(1243.22, 1245.84): ' And then I can run DAG relaxation', (1245.84, 1249.88): ' on this graph in linear time to get shortest paths.'}, {(1249.88, 1252.14): " So that's an approach."}, {(1252.14, 1254.44): ' If I knew the ordering of the vertices', (1254.44, 1259.02): ' in increasing distance, then I could use DAG relaxation.'}, {(1259.02, 1261.74): " So we're going to use both of these observations."}, {(1261.74, 1264.72): " That's how we're going to solve this single-source shortest", (1264.72, 1266.06): " problem with non-negative weights using Dijkstra. these observations, that's how we're going to solve this single-source shortest problem", (1266.06, 1268.82): ' with non-negative weights using Dijkstra.'}, {(1268.82, 1272.6): " So that's finally now where we're coming to."}, {(1272.6, 1276.24): ' Sorry, I missed a case here when I was writing up my notes.'}, {(1276.24, 1278.72): ' And I tried to fix it live.'}, {(1278.72, 1280.64): ' And hopefully, you guys followed me.'}, {(1280.64, 1281.14): ' OK.'}, {(1283.64, 1288.14): " Dijkstra's algorithm."}, {(1292.26, 1293.66): ' Did I spell that right?', (1293.66, 1295.06): ' Kind of.'}, {(1295.06, 1297.18): ' OK, what?', (1297.18, 1298.68): ' Dijkstra.'}, {(1302.48, 1309.26): ' OK, now Dijkstra was this Dutch computer scientist.'}, {(1309.26, 1310.68): ' This is him.'}, {(1310.68, 1311.36): ' Pretty famous.'}, {(1311.36, 1316.98): ' He wrote a monograph on why programming languages should', (1316.98, 1321.22): ' start with zero indexing as opposed to one indexing.'}, {(1321.22, 1322.26): ' So I like him.'}, {(1322.26, 1330.72): ' But in particular, he designed this very nice generalization of BFS for weighted graphs.'}, {(1330.72, 1335.28): " But maybe I didn't spell this right, because when he writes his name, he writes it with", (1335.28, 1337.86): ' a y with a dash over it.'}, {(1337.86, 1348.3): ' So in reality, on a Dutch typewriter, you might have a character that looks like this.'}, {(1348.4, 1350.54): ' Why? With a numloud on top of it.'}, {(1351.94, 1372.12): " But on an English keyboard, this looks pretty similar to an ij. So in a lot of manuscripts, we write it as d i j. There's no j sound in Dijkstra."}, {(1372.54, 1378.28): " It's coming from this y here. So that's an interesting way to remember how to spell Dijkstra."}, {(1378.28, 1385.06): ' But the basic idea behind Dijkstra is the following idea.'}, {(1389.84, 1411.76): ' Relax edges from vertices in increasing distance from source.'}, {(1411.76, 1414.14): ' OK, this is the same kind of difficulty', (1414.14, 1420.34): ' we had before when we were trying to generalize BFS.'}, {(1420.34, 1425.3): ' So how do we know what the next vertex is', (1425.3, 1427.58): ' with increasing distance to s?', (1427.58, 1443.42): ' Well, the second idea is find next vertex efficiently', (1443.42, 1448.44): ' using a data structure.'}, {(1450.14, 1454.0): " And the data structure we're going to use is something I like to call a changeable priority queue."}, {(1462.86, 1472.0): ' So this is a little different than a normal priority queue that we had at the end of our', (1472.0, 1476.22): ' data structures unit.'}, {(1476.22, 1479.44): ' This changeable priority queue has three operations.'}, {(1479.44, 1485.7): " We're going to say it's a Q. We can build it", (1485.7, 1489.34): ' on an iterable set of items.'}, {(1489.34, 1494.68): ' Just stick n items in there.'}, {(1494.68, 1506.72): ' We can delete min from the Q. OK, this is the same now as the priority queue.'}, {(1506.72, 1509.22): " It's this third operation that's going to be different."}, {(1512.9, 1522.44): ' Decrease the key of an item that has id id.'}, {(1522.44, 1525.7): ' OK, so this is a little strange.'}, {(1525.7, 1527.92): ' What the heck is this id?', (1527.92, 1530.5): ' With a changeable priority queue,', (1530.5, 1534.16): ' each of our items has two values instead of one value.'}, {(1534.16, 1539.86): ' It has a key, but it also, on which the priority queue is', (1539.86, 1545.0): ' deleting the min item with the minimum key,', (1545.18, 1548.6): ' but also each item has an ID associated with it,', (1548.6, 1551.86): ' a unique integer, okay?', (1551.86, 1556.78): ' So that when we perform this operation decrease key,', (1556.78, 1561.68): ' it can find some item in our data structure', (1561.68, 1570.46): " with a given ID, and if it's contained there, it's going to change its key to some smaller value, k."}, {(1571.46, 1571.72): ' Okay?', (1572.26, 1575.58): " And don't worry about the edge cases here."}, {(1575.9, 1578.92): " We're always going to make sure this k is going to be smaller", (1578.92, 1582.3): ' than whatever that key was to begin with.'}, {(1582.44, 1582.72): ' All right?', (1582.92, 1588.68): ' So this is really kind of a funky operation.'}, {(1588.68, 1594.62): ' If I had a priority queue, not a changeable priority queue, but I had a priority queue', (1594.62, 1602.18): ' and I wanted to implement a changeable priority queue, how could I do it?', (1602.18, 1609.88): ' Well, a regular priority queue is already going to get me these two operations.'}, {(1609.88, 1610.88): " It's just this one."}, {(1610.88, 1628.76): ' I essentially need to find something by an ID and then update its key. how to implement this is going to be to use a regular priority', (1628.76, 1629.26): ' queue.'}, {(1635.28, 1637.82): " I'm going to call it q prime."}, {(1637.82, 1645.7): " And I'm going to cross-link it with dictionary d."}, {(1650.66, 1654.24): ' So these are just regular priority queue on my items', (1654.24, 1658.64): ' that has the key as defined above.'}, {(1658.64, 1661.28): " But I'm going to cross-link it with a dictionary,", (1661.28, 1666.86): ' a dictionary that maps IDs to their location in the priority queue.'}, {(1666.86, 1670.6): " We've done this many times in the data structures section."}, {(1670.6, 1673.82): " We're trying to cross-link to data structures", (1673.82, 1678.62): ' to make a query on a different type of key', (1678.62, 1681.86): ' to find its place in another data structure.'}, {(1681.86, 1692.5): ' So if we had a priority queue and a dictionary, we could do this stuff pretty fast.'}, {(1692.5, 1697.8): " In particular, I'm going to assume that our IDs of our vertices are the integers between", (1697.8, 1700.02): ' 0 and v minus 1.'}, {(1700.02, 1707.06): ' And so for my dictionary, I could get constant time looking up of that ID by using what data', (1707.06, 1712.46): ' structure?', (1712.46, 1721.02): ' We could get expected constant time if we used a hash table.'}, {(1721.02, 1728.86): ' But if we knew that our vertex IDs were just the numbers from 0 to v minus 1, we could', (1728.86, 1734.62): ' get rid of that expected time by using a direct access array.'}, {(1734.62, 1735.62): ' Great.'}, {(1735.62, 1737.96): " OK, so that's the assumption."}, {(1737.96, 1747.32): " And so really, the name of the game here is to choose a priority queue here that's going to make these things fast when", (1747.32, 1748.86): ' we start to look at Dijkstra.'}, {(1748.86, 1752.52): " So we're going to use this data structure", (1752.52, 1758.46): ' to keep track of our distance estimates', (1758.46, 1762.22): ' to all of the vertices away from S.'}, {(1762.22, 1765.22): " So this is Dijkstra's algorithm."}, {(1765.22, 1767.88): ' OK, set.'}, {(1767.88, 1771.4): ' So same initialization step.'}, {(1771.4, 1775.76): " We're going to set our, this is a Desta's estimate, d, not", (1775.76, 1777.44): ' delta.'}, {(1777.44, 1780.18): " We're going to want the d's to be our deltas", (1780.18, 1781.52): ' at the end of the algorithm.'}, {(1781.52, 1783.54): " That's what we're going to have to prove."}, {(1783.54, 1787.8): ' So we first set all of them to infinity,', (1787.8, 1796.0): ' and then set d of ss equal to 0.'}, {(1796.0, 1798.6): " And here, we're never going to update it again,", (1798.6, 1802.58): ' because our shortest distances in a graph', (1802.58, 1806.56): " with non-negative edge weights certainly can't go below 0."}, {(1810.48, 1839.28): " Now we build our changeable priority queue with an item, I'm going to say an item is x is represented by a tuple of its ID, and", (1839.28, 1848.76): ' then its key, just for brevity here, with an item v, d of sv.'}, {(1849.12, 1852.7): " So I'm going to be storing in my changeable priority queue", (1852.7, 1858.12): ' the vertex label and its shortest path distance estimate,', (1858.36, 1858.6): ' d.'}, {(1859.04, 1861.06): " And that's going to be the key, the minimum", (1861.06, 1869.3): " that I'm going to be querying on for each v", (1869.3, 1873.42): " and v. So I'm going to build that thing."}, {(1873.42, 1878.18): " It's going to then have all of my vertices in my graph."}, {(1878.18, 1913.08): ' Then while my changeable priority queue still has items, not empty. U d s u. So some item such that its distance is minimized', (1913.08, 1921.88): ' from Q that has minimum distance.'}, {(1925.82, 1929.36): " So I'm going to look at all the things in my priority queue."}, {(1929.36, 1931.82): " At the start, it's just going to be s, right?", (1931.82, 1936.2): ' Because everything has shortest path distance estimate infinite', (1936.2, 1937.06): ' except for s.'}, {(1937.06, 1938.9): " And so that's clearly the smallest."}, {(1938.9, 1941.54): " OK, so I'm going to remove that from my queue."}, {(1941.54, 1943.1): " And then I'm going to process it."}, {(1943.1, 1945.16): ' OK, how am I going to process it?', (1945.16, 1948.58): " It's the exact same kind of thing as DAG relaxation."}, {(1948.58, 1951.24): " I'm going to relax all its outgoing edges."}, {(1951.24, 1969.78): " So just for completeness, for v in the outgoing adjacencies of u, I'm going to relax."}, {(1969.78, 1972.84): ' Oh, sorry.'}, {(1972.84, 1978.82): ' We have to check whether we can relax it.'}, {(1978.82, 1989.72): ' Basically, if the shortest path distance estimate to v', (1989.72, 2002.76): ' is greater than going to u first and then crossing that edge,', (2002.76, 2004.82): ' if going through that is better, this', (2004.82, 2007.76): ' is violating our triangle inequality.'}, {(2007.76, 2015.84): ' And so we relax edge u, v. And by that,', (2015.84, 2022.3): ' we mean set this thing to be equal to that thing.'}, {(2022.3, 2022.92): ' Right?', (2022.92, 2024.34): " That's what we meant by relax."}, {(2024.34, 2027.02): ' And then we have one other thing to do.'}, {(2027.02, 2033.62): " We've changed these distance estimates,", (2033.62, 2036.24): " but our queue doesn't know that we changed these things."}, {(2036.24, 2042.12): " We added these items in here, but it doesn't know", (2042.12, 2048.46): ' that my distances have changed. So we have to tell the Q to remember', (2048.46, 2075.62): ' to change its keysv, the one', (2075.62, 2077.58): ' that I just decreased here.'}, {(2077.58, 2079.98): ' And I know that I decreased it because I set it', (2079.98, 2081.0): ' to a smaller value.'}, {(2081.0, 2082.48): ' That makes sense.'}, {(2082.48, 2085.94): " All right, so that's Dijkstra."}, {(2085.94, 2090.58): " Let's run it on an example."}, {(2090.58, 2093.72): " So here's an example."}, {(2093.72, 2096.9): ' I have a directed graph.'}, {(2096.9, 2098.18): ' It does contain cycles.'}, {(2098.18, 2101.62): ' In particular, here are some cycles.'}, {(2101.62, 2105.34): ' I think those are the main ones.'}, {(2105.34, 2110.04): ' There are definitely cycles in this graph.'}, {(2110.04, 2113.32): ' But as you see, all of the weights are non-negative.'}, {(2113.32, 2116.12): " In particular, they're positive, actually."}, {(2116.12, 2121.44): " It's going to be just helpful in writing out this example."}, {(2121.44, 2126.3): " So let's run Dijkstra on this graph. Okay, first we initialize", (2126.3, 2132.1): " and we set the shortest path distance. I'm gonna label it in white here to all", (2132.1, 2135.48): " of the things and I'm going to, as I update it, I'm just gonna cross them out", (2135.48, 2140.88): " and write a new number. Okay, so that's what it is at the start, right? That's", (2140.88, 2145.36): " initialization, that's after step one. And then I stick things into my queue."}, {(2145.36, 2156.4): " What's in my queue? Here's my queue, right? It's everything, right? It's vertices S, A, B, C, D."}, {(2157.6, 2165.16): " Okay? I got five items in my queue. Really, it's the item pair with its shortest distance", (2165.16, 2165.74): ' estimate.'}, {(2165.74, 2168.44): " I'm just not going to rewrite that here."}, {(2168.44, 2171.32): " So the idea here is, what's the while loop?", (2171.32, 2172.88): ' OK, queue is not empty.'}, {(2172.88, 2173.98): ' Great.'}, {(2173.98, 2177.58): " We're going to delete the one with the smallest distance", (2177.58, 2179.92): ' estimate, which is s.'}, {(2179.92, 2181.22): ' Right, yeah.'}, {(2181.22, 2186.96): ' So I remove that, and then I relax edges out of s.'}, {(2186.96, 2190.8): ' So I relax edge here to a.'}, {(2190.8, 2192.94): " That's better than the distance estimate."}, {(2192.94, 2196.44): ' 10 is better than the distance estimate infinite.'}, {(2196.44, 2199.74): " So I'm going to change this to 10."}, {(2199.74, 2202.02): " And then here's another outgoing edge."}, {(2202.02, 2204.9): ' 3 is better than infinite.'}, {(2204.9, 2209.5): " So I'm going to change its delta to 3."}, {(2209.5, 2210.3): ' OK.'}, {(2210.3, 2214.6): ' So now I go back in here and I change the distance estimates', (2214.6, 2218.14): ' associated with my Q. OK?', (2218.14, 2221.86): ' Now, next step of the algorithm, S is done.'}, {(2221.86, 2226.44): " I've processed everything distance 0 away."}, {(2226.44, 2228.32): " But I'm now going to use my priority queue", (2228.32, 2232.56): ' to say, which of my vertices has the shortest distance estimate', (2232.56, 2234.56): ' now?', (2234.56, 2236.76): ' So which one is it?', (2236.76, 2238.24): ' A, B, or C, or D?', (2240.84, 2244.34): " Yeah, it's 3, C. 3 is smaller than 10."}, {(2244.34, 2248.0): ' So Q is going to magically delete C for me.'}, {(2248.0, 2255.0): " Tell me what that is. And now I'm going to process that, right? I've now changed my boundary to this."}, {(2255.0, 2267.18): " OK? And now I relax edges out of C. So here's an edge out of C. That's a 4. A 4 plus the 3 is smaller than 10."}, {(2267.18, 2269.5): ' So I update it.'}, {(2269.5, 2271.82): ' 3 plus 8 is 11.'}, {(2271.82, 2273.18): " That's smaller than infinite."}, {(2273.18, 2274.46): ' So I update it.'}, {(2274.46, 2276.52): ' I relax.'}, {(2276.52, 2279.46): ' 3 plus 2 is smaller than infinite.'}, {(2279.46, 2281.76): ' So I relax that as well.'}, {(2281.76, 2283.16): ' OK?', (2283.16, 2286.06): ' Now of the things still left in my queue,', (2286.06, 2289.1): " I'm actually going to remove it from my queue", (2289.1, 2290.28): ' instead of crossing it out.'}, {(2290.28, 2290.98): " Maybe that's better."}, {(2293.72, 2296.42): ' Of the vertices still left in my queue,', (2296.42, 2298.92): ' which has smallest distance?', (2298.92, 2300.8): ' Yeah, d.'}, {(2300.8, 2302.88): ' d has 5, 7 or 11.'}, {(2302.88, 2304.36): ' 5 is the smallest.'}, {(2304.36, 2307.22): ' So I remove d from my queue, and I relax edges from it.'}, {(2307.76, 2311.8): ' And now my boundary looks something like this.'}, {(2312.64, 2312.86): ' OK?', (2313.56, 2314.9): ' I relax edges out of it.'}, {(2315.16, 2317.4): " 5 plus 5, that's 10."}, {(2317.62, 2320.94): " 10 is smaller than 11, so that's a 10."}, {(2322.26, 2324.82): " And that's the only outgoing edge from d."}, {(2327.98, 2333.7): " So I'm done. And then the last, 7 is smaller than 10."}, {(2333.7, 2349.2): ' I relax edges out of A. A to B, 7 plus 2 is smaller than 10.'}, {(2350.26, 2350.38): " And now I'm done."}, {(2355.24, 2362.16): ' So what I did every time I removed S, or I removed a vertex, I set its shortest path distance to the last value I assigned to it.'}, {(2362.28, 2363.58): ' So this was then 3.'}, {(2369.66, 2373.14): ' And then A was 7, B was 9, and then D was 5.'}, {(2373.14, 2375.42): " So that's Dijkstra in action."}, {(2375.42, 2379.28): ' It seems like these are the shortest path distances.'}, {(2379.28, 2380.82): ' But how do we prove that?', (2380.82, 2383.94): ' Did it do the right thing?', (2383.94, 2385.64): " Well, let's find out."}, {(2385.64, 2389.26): " So that's what we're going to spend some time on right now,", (2389.26, 2391.76): " is talking about the correctness of Dijkstra's algorithm."}, {(2397.68, 2406.26): ' OK, correctness follows from two main observations.'}, {(2406.26, 2410.06): " So the claim here that we're trying to prove", (2410.06, 2418.54): ' is that d of s equals the delta s.'}, {(2418.54, 2423.84): ' So the estimates equal the shortest path distances', (2423.84, 2426.66): ' at the end of Dijkstra', (2426.66, 2434.0): ' for all v and v at n.'}, {(2437.54, 2440.08): ' And this is going to follow from two observations.'}, {(2440.08, 2456.68): ' So the proof here, first, if ever relaxation sets d of s', (2456.68, 2461.16): ' of v, it sets the estimate equal to the shortest path distance.'}, {(2461.16, 2473.32): ' If it ever does that, I argue to you that still true at end.'}, {(2475.96, 2478.56): " OK, that's not a very strong statement, right?", (2479.1, 2485.04): ' This is saying if I ever set the distance estimate to the true distance,', (2485.04, 2488.54): " I'm never going to set it to a different value later on."}, {(2488.54, 2489.54): ' And why is that?', (2492.14, 2496.38): ' Well, relaxation only ever decreases the distance.'}, {(2499.58, 2512.24): ' Relaxation only decreases dSv.'}, {(2512.24, 2522.32): ' But we proved in lecture 11, so two lectures ago, that relaxation is safe.'}, {(2522.32, 2537.3): ' And what does safe mean? Safe means that relaxation will only ever change these distance estimates to be either', (2537.3, 2551.52): ' infinite, right? a path to my vertex, or it was the length of some path to v.'}, {(2551.52, 2558.76): ' Length of some path.'}, {(2558.76, 2562.24): ' OK, so what does that mean?', (2562.24, 2566.08): " It only decreases, but it's always", (2566.08, 2569.26): ' the length of some path to v. So if this', (2569.26, 2571.5): ' is the length of the shortest path to v,', (2571.5, 2574.22): ' I could never set it to a smaller length', (2574.22, 2577.0): ' because there are no paths with shorter distance.'}, {(2577.0, 2578.6): " That's the kind of little point."}, {(2578.6, 2582.82): " So with this observation, I'm going", (2582.82, 2585.56): ' to argue this final claim.'}, {(2585.56, 2614.76): ' It suffices to show that my estimate equals the shortest distance when v is removed from the q.'}, {(2614.76, 2621.78): ' And since I remove every vertex from the q in this while loop,', (2621.78, 2625.44): ' I will eventually set all of the distance estimates', (2625.44, 2629.82): " to the real distance and we'll be golden."}, {(2629.82, 2630.86): ' Happy days.'}, {(2630.86, 2634.28): " All right, so we'll be done if we can prove that statement."}, {(2634.28, 2637.88): " All right, so we're going to prove this by induction,", (2637.88, 2661.96): ' obviously. Reduction on first k vertices removed from the Q.'}, {(2661.96, 2668.28): " So the Q, we're popping vertices from this q in some order."}, {(2668.28, 2674.88): " So I'm going to just argue that this claim is true for the first k."}, {(2674.88, 2678.32): " Clearly that's true for k equals 1."}, {(2678.32, 2684.7): ' Base case k equals 1.'}, {(2684.7, 2685.64): ' What is k equals 1. What is k equals 1?', (2685.64, 2688.34): ' That means the first vertex that I pop', (2688.34, 2691.0): ' has this property, which is definitely true,', (2691.0, 2694.86): ' because we set the shortest path distance to s to be 0.'}, {(2694.86, 2695.52): " That's all good."}, {(2698.14, 2709.7): ' Now we have our inductive step. OK?', (2709.7, 2730.58): " Assume it's true for k less than k prime. And let's let v prime be k prime vertex popped."}, {(2733.48, 2733.98): ' v prime.'}, {(2736.56, 2749.0): " And now let's look at some shortest path from s to V prime."}, {(2749.0, 2753.24): ' So we got the shortest path from S to V prime.'}, {(2753.24, 2754.28): ' It exists.'}, {(2754.28, 2756.42): ' V prime is accessible.'}, {(2756.42, 2760.18): " Let's say we pruned our graph to be only the things accessible", (2760.18, 2764.48): ' from S so that, yeah, there exists a shortest path', (2764.48, 2765.08): ' to V prime.'}, {(2765.76, 2765.88): ' OK?', (2767.22, 2770.76): " And now let's think about these vertices."}, {(2771.76, 2776.76): ' Some of them were removed from the queue, and some of them were not.'}, {(2777.28, 2777.36): ' Right?', (2777.62, 2779.22): ' s was definitely removed from the queue.'}, {(2779.76, 2780.36): ' Right?', (2780.98, 2783.98): ' But some of these other vertices might not be.'}, {(2784.66, 2784.98): ' Right?', (2784.98, 2785.2): ' I want to be able to induct on this path, in particular the vertex before me, Right? But some of these other vertices might not be. Right?', (2785.2, 2790.0): ' I want to be able to induct on this path, in particular the vertex before me,', (2790.3, 2795.0): ' so that I can say that when I removed it and I relaxed the edge to v prime,', (2795.8, 2796.9): " then we're all golden."}, {(2797.2, 2797.7): ' Right?', (2799.2, 2800.6): ' But that might not be the case.'}, {(2800.6, 2808.7): ' There could be a vertex, the vertex preceding me in the graph in this shortest path that was not popped from Q.'}, {(2808.7, 2812.78): ' I need to argue that it was or some other thing.'}, {(2812.78, 2821.86): " So let's consider the first vertex in this path from S", (2821.86, 2825.34): " to V. I'm going to call it y, I think."}, {(2825.34, 2825.84): ' Yeah.'}, {(2829.22, 2834.34): ' A vertex y that is not in q.'}, {(2834.34, 2835.54): ' Right?', (2835.54, 2841.3): ' After I pop v prime, this is the first or before I pop v prime,', (2841.3, 2842.42): ' y is not in the q.'}, {(2842.42, 2845.78): ' Now, these might be the same vertex', (2845.78, 2851.82): ' if all of the preceding ones on this path were in the queue.'}, {(2851.82, 2855.3): " But in particular, we're going to look at this guy."}, {(2855.3, 2859.04): ' And say its predecessor is x in the path.'}, {(2859.04, 2861.64): ' Well, what do I know?', (2861.64, 2865.5): ' I know that x is in the queue.'}, {(2865.5, 2872.42): ' Everything here was popped from the queue, not in.'}, {(2875.72, 2879.8): ' Which means that by induction, the shortest path distance', (2879.8, 2881.58): ' was set here correctly.'}, {(2881.58, 2887.98): ' So that the distance estimate at y', (2887.98, 2896.68): " can't be bigger than the shortest path to x plus wxy."}, {(2900.56, 2903.3): ' But this is on the shortest path to y,', (2903.3, 2907.36): ' because subpaths of shortest paths are shortest paths.'}, {(2907.36, 2912.48): ' So this has to equal dSy, the distance to y.'}, {(2912.48, 2915.18): ' So actually, y is all good here.'}, {(2915.18, 2919.14): " And so if v prime were y, we'd be done."}, {(2919.14, 2923.04): " That's the same argument as dag relaxation."}, {(2923.04, 2926.02): ' But we need to prove something about v prime.'}, {(2926.02, 2930.2): ' Well, because we have non-negative weights,', (2930.2, 2933.88): ' the distance to v prime has to be at least as big', (2933.88, 2936.98): " as this distance because it's a subpath."}, {(2936.98, 2941.44): ' So this has to be less than or equal to the true distance', (2941.44, 2951.1): ' to v prime because of non-negative weights,', (2951.1, 2955.04): ' because the weights are non-negative.'}, {(2955.04, 2958.44): ' But because relaxation is safe, we', (2958.44, 2961.14): ' know that our distance estimate for v prime', (2961.14, 2968.32): ' has to be at least the shortest path distance.'}, {(2971.5, 2976.76): " This is because it's safe. This is weights are greater than or equal to 0."}, {(2980.06, 2983.86): " The last step here is that because we're", (2983.86, 2988.28): ' popping the minimum from our priority queue,', (2988.28, 2991.46): ' the thing with the smallest shortest path distance,', (2991.46, 2996.3): ' this has to be less than or equal to the shortest path', (2996.3, 2999.44): ' distance estimate to y.'}, {(2999.44, 3002.4): ' Because this is the smallest among all such vertices', (3002.4, 3003.18): ' in my queue.'}, {(3005.7, 3007.58): ' But these are the same value.'}, {(3007.58, 3010.2): ' So everything between here is the same value.'}, {(3010.2, 3014.36): ' In particular, the estimate here is', (3014.36, 3017.58): ' equal to my true shortest path distance, which is exactly', (3017.58, 3018.84): " what we're trying to prove."}, {(3018.84, 3021.42): " So that's why Dijkstra is correct."}, {(3021.42, 3028.94): ' And spend the last five minutes on the running time of Dijkstra.'}, {(3028.94, 3040.44): ' We set this up so that we did everything in terms of these Q operations.'}, {(3040.44, 3042.2): ' So we have these Q operations.'}, {(3042.2, 3048.66): " We have three of them. I'm going to say if I have a build operation,", (3048.66, 3050.5): " let's say it takes B time."}, {(3050.5, 3053.18): " Delete min, I'm going to say it takes M time."}, {(3053.18, 3057.34): " And this decrease key, I'm going to say it takes D time."}, {(3057.34, 3059.32): ' So what is the running time of Dijkstra?', (3059.32, 3063.3): ' If I take a look at that algorithm over there,', (3063.3, 3070.0): " well, I guess let's switch these back up again."}, {(3070.0, 3071.76): ' OK, so what does this do?', (3071.76, 3072.6): ' We build once.'}, {(3075.32, 3078.86): ' Then we delete the minimum from the queue.'}, {(3078.86, 3081.06): ' How many times?', (3081.06, 3082.06): ' V times.'}, {(3082.06, 3086.48): ' We remove every vertex from our queue.'}, {(3086.48, 3090.92): ' Then for every possible edge, we may', (3090.92, 3095.4): ' need to relax and decrease the key in our queue', (3095.4, 3098.32): ' once for every outgoing edge.'}, {(3098.32, 3110.82): ' OK? So the running time is b plus v times m plus e times d.'}, {(3112.06, 3112.32): ' Right?', (3114.44, 3115.04): ' OK.'}, {(3115.46, 3117.84): ' So how could we implement this priority queue?', (3118.32, 3122.54): ' Well, if we use the stupidest priority queue in the world,', (3123.86, 3126.06): " here's a list of different implementations", (3126.06, 3128.62): ' we could have for our priority queues.'}, {(3128.62, 3132.9): ' And when I say priority queue, I mean this priority queue.'}, {(3132.9, 3135.34): " We're already implementing the changeable priority queue", (3135.34, 3138.86): " by linking it with a dictionary that's efficient."}, {(3138.86, 3142.28): ' If I just use an array, I can find the min in linear time,', (3142.28, 3144.06): ' sure.'}, {(3144.06, 3147.08): " And I don't have to update that array in any way."}, {(3147.08, 3154.62): ' I mean, I can just keep the distances in my direct access array.'}, {(3154.62, 3156.52): " I don't have to store a separate data structure."}, {(3156.52, 3160.34): ' I just store the distances in my direct access array D.'}, {(3160.34, 3163.36): ' And so I can find it in constant time.'}, {(3163.36, 3165.06): ' And I can update the values stored there.'}, {(3165.26, 3166.6): ' And then whenever I want the minimum,', (3166.76, 3168.2): ' I can just loop through the whole thing.'}, {(3168.96, 3171.34): ' So that gives me a really fast decrease key,', (3171.9, 3173.82): ' but slow delete min.'}, {(3174.12, 3176.64): ' But if we take a look at the running time bound here,', (3178.76, 3181.26): ' we get something, if we replace n with v,', (3181.5, 3184.16): ' we get a quadratic time algorithm', (3184.16, 3187.3): ' in the number of vertices, which', (3187.3, 3193.76): " for a dense graph, this is in linear time. That's actually pretty good. Dense meaning", (3193.76, 3199.9): " that I have at least a quadratic number of vertices. So that's actually really good."}, {(3199.9, 3205.46): " And it's the stupidest possible data structure we could use for this priority queue."}, {(3205.46, 3209.48): ' Now, we can do a little better, actually, for not dense.'}, {(3209.48, 3212.84): ' I mean, for sparse graphs, where the number of edges', (3212.84, 3219.18): ' is at most v, then this is pretty bad.'}, {(3219.18, 3220.18): " It's quadratic."}, {(3220.18, 3221.92): ' We want to do something a little better.'}, {(3221.92, 3225.34): " Now, if we're sparse, a binary heap", (3225.34, 3229.3): ' can delete min in logarithmic time.'}, {(3229.3, 3233.14): ' But it can actually, if I know where I am in the heap', (3233.14, 3239.14): " and I decrease the key and I'm in a min heap,", (3239.14, 3242.94): ' I can just swap with my parent upwards in the tree in log n', (3242.94, 3245.76): ' time and rebalancebalance the refix', (3245.76, 3251.64): ' the binary heat property and so I can do that in logarithmic time okay and if I', (3251.64, 3261.14): ' do that and I put it into this formula I actually get n or V plus V times log V', (3261.14, 3265.1): ' plus e times log v.'}, {(3265.1, 3268.2): " And so that's going to give me e log v", (3268.2, 3272.0): " if I'm assuming that I'm first pruning out", (3272.0, 3274.2): ' all of the things not connected to me,', (3274.2, 3277.68): ' then e asymptotically upper bounds v,', (3277.68, 3282.44): ' and I get this e log v running time, which is pretty good.'}, {(3282.44, 3288.38): " That's just an extra log factor on linear."}, {(3294.88, 3297.96): " Now, there's an even better, well, better is hard to say. Really, there's a different data structure", (3297.96, 3303.94): ' that achieves both bounds for sparse and dense graphs', (3303.94, 3309.66): ' and everything in between, it gives us an E plus V log V running time', (3309.66, 3310.16): ' bound.'}, {(3310.16, 3312.32): ' This data structure is called the Fibonacci heap.'}, {(3312.32, 3315.76): " We're not going to talk about it in 6.006."}, {(3315.76, 3316.68): ' They talk about it.'}, {(3316.68, 3320.68): ' You can look at chapter 19 in CLRS, or you can look at,', (3320.68, 3323.52): ' I think they talk about it in 6.854,', (3323.52, 3326.18): " if you're interested in learning about Fibonacci heaps."}, {(3326.18, 3327.8): ' But these are almost never, I mean,', (3327.8, 3330.68): ' they get good theoretical bounds.'}, {(3330.68, 3334.28): ' So what you want to say is, whenever we give you a theory', (3334.28, 3336.96): ' problem where you might want to use Dijkstra,', (3336.96, 3341.66): ' you want to use this theoretical running time', (3341.66, 3347.36): ' bound for your problem, a plus v log v.'}, {(3348.0, 3352.14): ' But if you happen to know that your graph is sparse or dense,', (3353.52, 3359.18): ' just using an array or a heap is going to get you just as good of a running time,', (3359.44, 3361.68): ' very close to linear, right?', (3362.0, 3365.74): ' And so in practice, most people, when', (3365.74, 3367.68): " they're implementing a graph search algorithm,", (3367.68, 3370.82): ' they know if their graph is sparse or dense.'}, {(3370.82, 3373.62): ' And so they never bother implementing a Fibonacci heap,', (3373.62, 3376.24): ' which is a little complicated, OK?', (3376.24, 3379.62): ' So they are usually either in one of these first two cases,', (3379.62, 3383.44): ' where v squared is linear when your graph is dense,', (3383.44, 3387.6): " or we're very close to linear, e times log v,", (3387.6, 3391.56): ' which is v log v if your graph is sparse.'}, {(3391.56, 3395.14): " So that's the running time of Dijkstra."}, {(3400.36, 3405.0): " So far, we've gotten all of these nice bounds, right?", (3405.72, 3408.04): " Some special cases where we're,", (3408.04, 3411.18): " I mean, special cases where we're linear,", (3411.18, 3413.94): " Dijkstra where we're close to linear,", (3415.28, 3417.06): ' and Bellman-Ford, you know,', (3417.06, 3419.22): ' if we throw our hands up in the air,', (3419.22, 3421.6): ' there might be negative cycles in our graph.'}, {(3421.6, 3423.6): ' We gotta spend that quadratic running time bound.'}, {(3423.6, 3426.94): " Now there are faster algorithms, but this is the fastest we're going to teach you in", (3426.94, 3428.12): ' this class.'}, {(3428.12, 3433.26): " Now in the next lecture, we're going to be talking about all-pair shortest paths, and", (3433.26, 3435.18): " we'll pick it up next time."}]


document = Document(sample_document)
document2 = Document(sample_document)
document3 = Document(sample_document2)

print("Doc 3: ", document3)

print(document)


print("Sentence 1: ", document.sentences[0])
print("Sentence at 5.0s: ", document.find_sentence(5.0))
print("Sentence fragment at 20.0s: ", document2.find_segment(20.0))
print("Document 2: ", document2)
print(
    "Start time of document 2, sentence at 12.7s: ",
    document2.find_sentence(12.70).start,
)
print(
    "End time of document 2, sentence segment at 12.7s: ",
    document2.find_segment(12.70).end,
)
